import os
import shutil
from pathlib import Path
import warnings

warnings.filterwarnings('ignore')

# å°è¯•å¯¼å…¥pyannote.audioï¼ˆæœ€é«˜ç²¾åº¦æ–¹æ¡ˆï¼‰
try:
    from pyannote.audio import Pipeline
    import torch

    PYANNOTE_AVAILABLE = True
except ImportError:
    PYANNOTE_AVAILABLE = False
    print("âš ï¸  æœªå®‰è£… pyannote.audioï¼Œå°†ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")
    print("   æ¨èå®‰è£…ä»¥è·å¾—æœ€é«˜ç²¾åº¦ï¼špip install pyannote.audio")

# å¤‡ç”¨æ–¹æ¡ˆï¼šspeechbrain
try:
    from speechbrain.pretrained import SpeakerRecognition

    SPEECHBRAIN_AVAILABLE = True
except ImportError:
    SPEECHBRAIN_AVAILABLE = False

# æœ€åå¤‡ç”¨æ–¹æ¡ˆï¼šåŸºäºMFCCçš„æ–¹æ³•
import librosa
import numpy as np
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler


class SpeakerDetector:
    """é«˜ç²¾åº¦è¯´è¯äººæ£€æµ‹å™¨"""

    def __init__(self, method='auto', hf_token=None):
        """
        åˆå§‹åŒ–æ£€æµ‹å™¨

        å‚æ•°:
            method: 'auto', 'pyannote', 'speechbrain', 'mfcc'
            hf_token: Hugging Face token (pyannoteæ–¹æ³•éœ€è¦)
        """
        self.method = method
        self.hf_token = hf_token
        self.pipeline = None
        self.speaker_model = None

        if method == 'auto':
            if PYANNOTE_AVAILABLE and hf_token:
                self.method = 'pyannote'
            elif SPEECHBRAIN_AVAILABLE:
                self.method = 'speechbrain'
            else:
                self.method = 'mfcc'

        self._initialize_model()

    def _initialize_model(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        if self.method == 'pyannote' and PYANNOTE_AVAILABLE:
            try:
                print("ğŸš€ åŠ è½½ pyannote.audio æ¨¡å‹ï¼ˆæœ€é«˜ç²¾åº¦ï¼‰...")
                self.pipeline = Pipeline.from_pretrained(
                    "pyannote/speaker-diarization-3.1",
                    use_auth_token=self.hf_token
                )
                # ä½¿ç”¨GPUåŠ é€Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if torch.cuda.is_available():
                    self.pipeline.to(torch.device("cuda"))
                    print("   âœ“ ä½¿ç”¨GPUåŠ é€Ÿ")
                print("   âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
            except Exception as e:
                print(f"   âœ— åŠ è½½å¤±è´¥: {e}")
                print("   â†’ åˆ‡æ¢åˆ°å¤‡ç”¨æ–¹æ¡ˆ")
                self.method = 'speechbrain' if SPEECHBRAIN_AVAILABLE else 'mfcc'
                self._initialize_model()

        elif self.method == 'speechbrain' and SPEECHBRAIN_AVAILABLE:
            try:
                print("ğŸš€ åŠ è½½ SpeechBrain æ¨¡å‹ï¼ˆé«˜ç²¾åº¦ï¼‰...")
                self.speaker_model = SpeakerRecognition.from_hparams(
                    source="speechbrain/spkrec-ecapa-voxceleb",
                    savedir="pretrained_models/spkrec-ecapa-voxceleb"
                )
                print("   âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
            except Exception as e:
                print(f"   âœ— åŠ è½½å¤±è´¥: {e}")
                print("   â†’ åˆ‡æ¢åˆ°åŸºç¡€æ–¹æ¡ˆ")
                self.method = 'mfcc'

        else:
            print("ğŸ”§ ä½¿ç”¨åŸºäºMFCCçš„æ£€æµ‹æ–¹æ³•ï¼ˆåŸºç¡€æ–¹æ¡ˆï¼‰")

    def detect_multiple_speakers(self, audio_path, min_duration=0.5):
        """
        æ£€æµ‹éŸ³é¢‘ä¸­æ˜¯å¦æœ‰å¤šä¸ªè¯´è¯äºº

        å‚æ•°:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            min_duration: æœ€å°è¯´è¯äººæŒç»­æ—¶é—´ï¼ˆç§’ï¼‰

        è¿”å›:
            (is_multi_speaker, num_speakers, confidence)
        """
        if self.method == 'pyannote' and self.pipeline:
            return self._detect_pyannote(audio_path, min_duration)
        elif self.method == 'speechbrain' and self.speaker_model:
            return self._detect_speechbrain(audio_path)
        else:
            return self._detect_mfcc(audio_path)

    def _detect_pyannote(self, audio_path, min_duration):
        """ä½¿ç”¨pyannote.audioæ£€æµ‹ï¼ˆæœ€é«˜ç²¾åº¦ï¼‰"""
        try:
            diarization = self.pipeline(audio_path, min_duration_on=min_duration)

            # è·å–å”¯ä¸€è¯´è¯äººæ•°é‡
            speakers = set()
            for turn, _, speaker in diarization.itertracks(yield_label=True):
                speakers.add(speaker)

            num_speakers = len(speakers)
            is_multi = num_speakers > 1
            confidence = 0.95  # pyannoteç²¾åº¦å¾ˆé«˜

            return is_multi, num_speakers, confidence

        except Exception as e:
            print(f"   âœ— Pyannoteæ£€æµ‹å¤±è´¥: {e}")
            return False, 1, 0.0

    def _detect_speechbrain(self, audio_path):
        """ä½¿ç”¨SpeechBrainæ£€æµ‹"""
        try:
            # åŠ è½½éŸ³é¢‘
            import torchaudio
            signal, fs = torchaudio.load(audio_path)

            # å°†éŸ³é¢‘åˆ†æˆå¤šä¸ªç‰‡æ®µ
            segment_length = int(fs * 3)  # 3ç§’ç‰‡æ®µ
            num_segments = max(1, signal.shape[1] // segment_length)

            embeddings = []
            for i in range(num_segments):
                start = i * segment_length
                end = min(start + segment_length, signal.shape[1])
                segment = signal[:, start:end]

                if segment.shape[1] < fs:  # è‡³å°‘1ç§’
                    continue

                # æå–è¯´è¯äººåµŒå…¥
                embedding = self.speaker_model.encode_batch(segment)
                embeddings.append(embedding.squeeze().cpu().numpy())

            if len(embeddings) < 2:
                return False, 1, 0.8

            embeddings = np.array(embeddings)

            # ä½¿ç”¨DBSCANèšç±»
            scaler = StandardScaler()
            embeddings_scaled = scaler.fit_transform(embeddings)

            clustering = DBSCAN(eps=0.5, min_samples=2).fit(embeddings_scaled)
            num_speakers = len(set(clustering.labels_)) - (1 if -1 in clustering.labels_ else 0)

            is_multi = num_speakers > 1
            confidence = 0.85

            return is_multi, max(1, num_speakers), confidence

        except Exception as e:
            print(f"   âœ— SpeechBrainæ£€æµ‹å¤±è´¥: {e}")
            return self._detect_mfcc(audio_path)

    def _detect_mfcc(self, audio_path):
        """ä½¿ç”¨MFCCç‰¹å¾æ£€æµ‹ï¼ˆåŸºç¡€æ–¹æ¡ˆï¼‰"""
        try:
            y, sr = librosa.load(audio_path, sr=16000)
            duration = librosa.get_duration(y=y, sr=sr)

            if duration < 1.0:
                return False, 1, 0.6

            # æå–MFCCç‰¹å¾
            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)

            # åˆ†æ®µåˆ†æ
            n_segments = min(20, int(duration))
            segment_length = mfccs.shape[1] // n_segments

            features = []
            for i in range(n_segments):
                start = i * segment_length
                end = min(start + segment_length, mfccs.shape[1])
                if end - start > 0:
                    segment_mfcc = mfccs[:, start:end].mean(axis=1)
                    features.append(segment_mfcc)

            if len(features) < 2:
                return False, 1, 0.6

            features = np.array(features)
            scaler = StandardScaler()
            features_scaled = scaler.fit_transform(features)

            # ä½¿ç”¨DBSCANèšç±»
            clustering = DBSCAN(eps=0.8, min_samples=2).fit(features_scaled)
            labels = clustering.labels_
            num_speakers = len(set(labels)) - (1 if -1 in labels else 0)

            is_multi = num_speakers > 1
            confidence = 0.7

            return is_multi, max(1, num_speakers), confidence

        except Exception as e:
            print(f"   âœ— MFCCæ£€æµ‹å¤±è´¥: {e}")
            return False, 1, 0.0


def process_audio_folder(source_folder, multi_speaker_folder="multi-speaker",
                         method='auto', hf_token=None, confidence_threshold=0.7):
    """
    å¤„ç†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰WAVæ–‡ä»¶

    å‚æ•°:
        source_folder: æºæ–‡ä»¶å¤¹è·¯å¾„
        multi_speaker_folder: å¤šè¯´è¯äººæ–‡ä»¶å­˜æ”¾çš„æ–‡ä»¶å¤¹åç§°
        method: æ£€æµ‹æ–¹æ³• ('auto', 'pyannote', 'speechbrain', 'mfcc')
        hf_token: Hugging Face token (ä½¿ç”¨pyannoteæ—¶å¿…éœ€)
        confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
    """
    source_path = Path(source_folder)

    if not source_path.exists():
        print(f"âŒ æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {source_folder}")
        return

    # åˆ›å»ºå¤šè¯´è¯äººæ–‡ä»¶å¤¹
    multi_speaker_path = source_path / multi_speaker_folder
    multi_speaker_path.mkdir(exist_ok=True)

    # è·å–æ‰€æœ‰WAVæ–‡ä»¶
    wav_files = list(source_path.glob("*.wav"))

    if not wav_files:
        print(f"âš ï¸  åœ¨ {source_folder} ä¸­æ²¡æœ‰æ‰¾åˆ°WAVæ–‡ä»¶")
        return

    print(f"\nğŸ“ æ‰¾åˆ° {len(wav_files)} ä¸ªWAVæ–‡ä»¶")
    print(f"ğŸ” å¼€å§‹æ£€æµ‹...\n")

    # åˆå§‹åŒ–æ£€æµ‹å™¨
    detector = SpeakerDetector(method=method, hf_token=hf_token)
    print(f"ğŸ“Š å½“å‰ä½¿ç”¨æ–¹æ³•: {detector.method.upper()}\n")

    multi_speaker_count = 0
    single_speaker_count = 0
    results = []

    for i, wav_file in enumerate(wav_files, 1):
        # è·³è¿‡å·²ç»åœ¨multi-speakeræ–‡ä»¶å¤¹ä¸­çš„æ–‡ä»¶
        if multi_speaker_folder in str(wav_file):
            continue

        print(f"[{i}/{len(wav_files)}] æ£€æµ‹: {wav_file.name}")

        is_multi, num_speakers, confidence = detector.detect_multiple_speakers(str(wav_file))

        result_info = {
            'file': wav_file.name,
            'is_multi': is_multi,
            'num_speakers': num_speakers,
            'confidence': confidence
        }
        results.append(result_info)

        print(f"   è¯´è¯äººæ•°: {num_speakers}, ç½®ä¿¡åº¦: {confidence:.2f}")

        if is_multi and confidence >= confidence_threshold:
            # ç§»åŠ¨åˆ°multi-speakeræ–‡ä»¶å¤¹
            dest_path = multi_speaker_path / wav_file.name
            shutil.move(str(wav_file), str(dest_path))
            print(f"   âœ… å¤šè¯´è¯äºº â†’ å·²ç§»åŠ¨åˆ° {multi_speaker_folder}/")
            multi_speaker_count += 1
        else:
            print(f"   â„¹ï¸  å•è¯´è¯äºº â†’ ä¿æŒåŸä½ç½®")
            single_speaker_count += 1

        print()

    print("=" * 60)
    print(f"âœ¨ å¤„ç†å®Œæˆ!")
    print(f"\nğŸ“Š ç»Ÿè®¡ç»“æœ:")
    print(f"   - å•è¯´è¯äººæ–‡ä»¶: {single_speaker_count}")
    print(f"   - å¤šè¯´è¯äººæ–‡ä»¶: {multi_speaker_count}")
    print(f"   - æ£€æµ‹æ–¹æ³•: {detector.method.upper()}")
    print(f"   - å¤šè¯´è¯äººæ–‡ä»¶å·²ç§»åŠ¨åˆ°: {multi_speaker_path}")

    # æ˜¾ç¤ºè¯¦ç»†ç»“æœ
    print(f"\nğŸ“‹ è¯¦ç»†ç»“æœ:")
    for result in results:
        status = "âœ“ å¤šè¯´è¯äºº" if result['is_multi'] else "  å•è¯´è¯äºº"
        print(f"   {status} | {result['num_speakers']}äºº | "
              f"ç½®ä¿¡åº¦:{result['confidence']:.2f} | {result['file']}")

    print("=" * 60)


if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ™ï¸  é«˜ç²¾åº¦å¤šè¯´è¯äººéŸ³é¢‘æ£€æµ‹å·¥å…·")
    print("=" * 60)

    # é€‰æ‹©æ£€æµ‹æ–¹æ³•
    print("\nå¯ç”¨çš„æ£€æµ‹æ–¹æ³•:")
    print("  1. auto      - è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ–¹æ³•ï¼ˆæ¨èï¼‰")
    print("  2. pyannote  - æœ€é«˜ç²¾åº¦ï¼ˆéœ€è¦Hugging Face tokenï¼‰")
    print("  3. speechbrain - é«˜ç²¾åº¦")
    print("  4. mfcc      - åŸºç¡€æ–¹æ³•")

    method_choice = input("\nè¯·é€‰æ‹©æ–¹æ³• (1-4ï¼Œé»˜è®¤1): ").strip() or "1"
    method_map = {"1": "auto", "2": "pyannote", "3": "speechbrain", "4": "mfcc"}
    method = method_map.get(method_choice, "auto")

    hf_token = None
    if method in ['auto', 'pyannote']:
        print("\nğŸ’¡ æç¤º: ä½¿ç”¨ pyannote.audio éœ€è¦ Hugging Face token")
        print("   è·å–æ–¹å¼: https://huggingface.co/settings/tokens")
        print("   ç„¶åéœ€è¦æ¥å—æ¨¡å‹è®¸å¯: https://huggingface.co/pyannote/speaker-diarization-3.1")
        hf_token = input("\nè¯·è¾“å…¥ Hugging Face token (å¯é€‰ï¼Œç›´æ¥å›è½¦è·³è¿‡): ").strip() or None

    folder_path = input("\nè¯·è¾“å…¥WAVæ–‡ä»¶æ‰€åœ¨çš„æ–‡ä»¶å¤¹è·¯å¾„: ").strip()

    if folder_path:
        process_audio_folder(
            folder_path,
            method=method,
            hf_token=hf_token,
            confidence_threshold=0.7
        )
    else:
        print("âŒ æœªè¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„")